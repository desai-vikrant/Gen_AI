{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb5e0a1",
   "metadata": {},
   "source": [
    "# BERT Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61779335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting PyPDF\n",
      "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (11.3.0)\n",
      "Collecting pdfminer.six==20251230 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-5.3.0-py3-none-win_amd64.whl.metadata (67 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six==20251230->pdfplumber) (3.4.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pytesseract) (25.0)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading cffi-2.0.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Downloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.3/6.6 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.6 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.4/6.6 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.5/6.6 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.5/6.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 5.0 MB/s  0:00:01\n",
      "Downloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.3/3.5 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.4/3.5 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.4/3.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 5.5 MB/s  0:00:00\n",
      "Downloading cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
      "Downloading pypdfium2-5.3.0-py3-none-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.0/3.1 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.1/3.1 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 5.3 MB/s  0:00:00\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pytesseract, pypdfium2, PyPDF, pycparser, pdf2image, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "\n",
      "   ---------------------------------------- 0/9 [pytesseract]\n",
      "   ---- ----------------------------------- 1/9 [pypdfium2]\n",
      "   ---- ----------------------------------- 1/9 [pypdfium2]\n",
      "   ---- ----------------------------------- 1/9 [pypdfium2]\n",
      "   ---- ----------------------------------- 1/9 [pypdfium2]\n",
      "   ---- ----------------------------------- 1/9 [pypdfium2]\n",
      "   ---- ----------------------------------- 1/9 [pypdfium2]\n",
      "   ---- ----------------------------------- 1/9 [pypdfium2]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   -------- ------------------------------- 2/9 [PyPDF]\n",
      "   ------------- -------------------------- 3/9 [pycparser]\n",
      "   ------------- -------------------------- 3/9 [pycparser]\n",
      "   ------------- -------------------------- 3/9 [pycparser]\n",
      "   ------------- -------------------------- 3/9 [pycparser]\n",
      "   ----------------- ---------------------- 4/9 [pdf2image]\n",
      "   ---------------------- ----------------- 5/9 [cffi]\n",
      "   ---------------------- ----------------- 5/9 [cffi]\n",
      "   ---------------------- ----------------- 5/9 [cffi]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   -------------------------- ------------- 6/9 [cryptography]\n",
      "   ------------------------------- -------- 7/9 [pdfminer.six]\n",
      "   ------------------------------- -------- 7/9 [pdfminer.six]\n",
      "   ------------------------------- -------- 7/9 [pdfminer.six]\n",
      "   ------------------------------- -------- 7/9 [pdfminer.six]\n",
      "   ------------------------------- -------- 7/9 [pdfminer.six]\n",
      "   ------------------------------- -------- 7/9 [pdfminer.six]\n",
      "   ------------------------------- -------- 7/9 [pdfminer.six]\n",
      "   ------------------------------- -------- 7/9 [pdfminer.six]\n",
      "   ----------------------------------- ---- 8/9 [pdfplumber]\n",
      "   ----------------------------------- ---- 8/9 [pdfplumber]\n",
      "   ----------------------------------- ---- 8/9 [pdfplumber]\n",
      "   ----------------------------------- ---- 8/9 [pdfplumber]\n",
      "   ---------------------------------------- 9/9 [pdfplumber]\n",
      "\n",
      "Successfully installed PyPDF-6.5.0 cffi-2.0.0 cryptography-46.0.3 pdf2image-1.17.0 pdfminer.six-20251230 pdfplumber-0.11.9 pycparser-2.23 pypdfium2-5.3.0 pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install pdfplumber PyPDF2 pdf2image pytesseract pillow\n",
    "\n",
    "    # pdfplumber normal PDFs ka text nikalne ke kaam aata hai.\n",
    "    # PyPDF2 PDF info read karne ke kaam aata hai.\n",
    "    # pdf2image scanned PDF pages ko image me convert karega.\n",
    "    # pytesseract OCR karega (text detect from images).\n",
    "    # Pillow image handling library hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a226ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Pages:  2\n",
      "\n",
      "[Page 1] Checking PDF page type....\n",
      "Detected: Searchable text PDF...\n",
      "\n",
      "[Page 2] Checking PDF page type....\n",
      "Detected: Searchable text PDF...\n",
      "Extracted Text: \n",
      "Vikrant Desai\n",
      "desaivikrant276@gmail.com 9503629601 Pune, Maharashtra, India Vikrant Desai \n",
      "Vikrant Desai Project Portfolio Website \n",
      "Career Objective\n",
      "Detail-oriented Data Analyst with hands-on experience in data exploration, reporting, testing, visualization, \n",
      "and documentation. Skilled in Python, SQL, Excel, and BI tools with strong analytical thinking, problem-\n",
      "solving ability, and client communication. Experienced in validating model outputs, preparing requirement-\n",
      "aligned summaries, and docum\n",
      "Done Text saved as output_text.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    combined_text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    num_pages = len(reader.pages)\n",
    "    print(\"Total Pages: \", num_pages)\n",
    "\n",
    "    for page_num in range(num_pages):\n",
    "        page = reader.pages[page_num]\n",
    "        extracted = page.extract_text()\n",
    "        print(f\"\\n[Page {page_num+1}] Checking PDF page type....\")\n",
    "\n",
    "        if extracted and len(extracted.strip()) > 20:\n",
    "            print(\"Detected: Searchable text PDF...\")\n",
    "            combined_text += extracted + \"\\n\"\n",
    "        else:\n",
    "            print(\"Detected: Image/Scanned PDF - WE apply OCR....\")\n",
    "            images = convert_from_path(pdf_path, first_page=page_num+1, last_page=page_num+1, poppler_path=r\"C:\\poppler\\poppler-25.12.0\\Library\\bin\")\n",
    "\n",
    "            for img in images:\n",
    "                text = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "                combined_text += text + \"\\n\"\n",
    "    return combined_text\n",
    "\n",
    "pdf_file = r\"/content/drive/MyDrive/Gen_AI/Gen_AI.pdf\"\n",
    "text_output = extract_text(pdf_file)\n",
    "\n",
    "with open(r\"C:\\Users\\HP\\Downloads\\output_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_output)\n",
    "\n",
    "print(\"Extracted Text: \")\n",
    "print(text_output[:500])\n",
    "print(\"Done Text saved as output_text.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1bb526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences:  25\n",
      "\n",
      "Example Sentences: \n",
      " ['vikrant desai desaivikrant276@gmail.com 9503629601 pune, maharashtra, india vikrant desai vikrant desai project portfolio website career objective detail-oriented data analyst with hands-on experience in data exploration, reporting, testing, visualization, and documentation.', 'skilled in python, sql, excel, and bi tools with strong analytical thinking, problem- solving ability, and client communication.', 'experienced in validating model outputs, preparing requirement- aligned summaries, and documenting insights for business decision-making.', 'passionate about improving usability, clarity, and data-driven decision support.', 'work experience internship: data science intern nexgen analytix duration: ongoing (started in may 2025) location: pune, india performed data cleaning, preprocessing, and exploratory data analysis (eda) on business datasets.', 'built analytical dashboards & reports using power bi, excel, tableau, and python for business insight support.', 'used sql for data extraction, joins, filters, aggregations, and reporting.', 'assisted in testing & evaluation of model outputs (accuracy, confusion matrix, rmse, etc).', 'prepared documentation, reporting summaries, and communicated findings for requirement understanding.', 'projects: project portfolio website 1) loan approval prediction gathered problem requirements and identified success metrics for loan eligibility.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def clean_raw_text(raw_text):\n",
    "    text = raw_text.lower()\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def split_into_sentences(clean_text):\n",
    "    sentences = sent_tokenize(clean_text)\n",
    "    return sentences\n",
    "\n",
    "with open(r\"C:\\Users\\HP\\Downloads\\output_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = f.read()\n",
    "\n",
    "cleaned = clean_raw_text(raw)\n",
    "sentences = split_into_sentences(cleaned)\n",
    "\n",
    "print(\"Total Sentences: \", len(sentences))\n",
    "print(\"\\nExample Sentences: \\n\", sentences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1dfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def chunk_sentences(sentences, chunk_size = 250):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        current_chunk.append(sent)\n",
    "\n",
    "        if sum(len(s.split()) for s in current_chunk) > chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db4fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks:  2\n",
      "\n",
      "Sample Chunk: \n",
      " vikrant desai desaivikrant276@gmail.com 9503629601 pune, maharashtra, india vikrant desai vikrant desai project portfolio website career objective detail-oriented data analyst with hands-on experience in data exploration, reporting, testing, visualization, and documentation. skilled in python, sql, excel, and bi tools with strong analytical thinking, problem- solving ability, and client communication. experienced in validating model outputs, preparing requirement- aligned summaries, and document\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_sentences(sentences)\n",
    "\n",
    "print(\"Total Chunks: \", len(chunks))\n",
    "print(\"\\nSample Chunk: \\n\", chunks[0][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c9205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Encoding chunks... please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (2, 384)\n",
      "‚ú® Embeddings saved as pdf_embeddings.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 1. Load BERT model (optimized for semantic search)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. Convert text chunks -> embeddings\n",
    "print(\"Encoding chunks... please wait...\")\n",
    "chunk_embeddings = model.encode(chunks, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# 3. Convert to numpy array for fast similarity search\n",
    "chunk_embeddings = np.array(chunk_embeddings)\n",
    "\n",
    "print(\"Embedding shape:\", chunk_embeddings.shape)\n",
    "\n",
    "# 4. Store chunks + embeddings for later QA\n",
    "with open(\"pdf_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"chunks\": chunks,\n",
    "        \"embeddings\": chunk_embeddings\n",
    "    }, f)\n",
    "\n",
    "print(\"‚ú® Embeddings saved as pdf_embeddings.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2814dfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 2 chunks\n",
      "Embedding shape: (2, 384)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"pdf_embeddings.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "chunks = data[\"chunks\"]\n",
    "chunk_embeddings = data[\"embeddings\"]\n",
    "\n",
    "print(\"Loaded:\", len(chunks), \"chunks\")\n",
    "print(\"Embedding shape:\", chunk_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d482232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb042445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, top_k=3):\n",
    "    # Encode question\n",
    "    q_embedding = model.encode([question])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(q_embedding, chunk_embeddings)[0]\n",
    "    \n",
    "    # Sort by similarity score descending\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    print(f\"\\nüîç Top {top_k} relevant chunks:\\n\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"üìå [Score: {similarities[idx]:.4f}]\")\n",
    "        print(chunks[idx][:500], \"...\")  # show first 500 chars\n",
    "        print(\"-\"*80)\n",
    "    \n",
    "    # Return results for further processing if needed\n",
    "    return [(idx, similarities[idx], chunks[idx]) for idx in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0302b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "# *** Change filename according to YOUR file ***\n",
    "# llm = GPT4All(\"Phi-3-mini-4k-instruct.Q4_0.gguf\", model_path=r\"C:\\Users\\HP\\AppData\\Local\\nomic.ai\\GPT4All\")\n",
    "llm = GPT4All(\"Phi-3-mini-4k-instruct.Q4_0.gguf\", \n",
    "              model_path=\"/content/drive/MyDrive/Gen_AI/Phi-3-mini-4k-instruct.Q4_0.gguf\")\n",
    "\n",
    "def generate_answer(question, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join([c[2] for c in retrieved_chunks])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Answer the question using ONLY the context below.\n",
    "If answer is not in context, say: \"Not available in the PDF.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in 5-6 lines:\n",
    "\"\"\"\n",
    "    print(\"Generating answer... (please wait)\")\n",
    "    response = llm.generate(prompt, max_tokens=300, temp=0.2)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20c9d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 3 relevant chunks:\n",
      "\n",
      "üìå [Score: 0.3939]\n",
      "vikrant desai desaivikrant276@gmail.com 9503629601 pune, maharashtra, india vikrant desai vikrant desai project portfolio website career objective detail-oriented data analyst with hands-on experience in data exploration, reporting, testing, visualization, and documentation. skilled in python, sql, excel, and bi tools with strong analytical thinking, problem- solving ability, and client communication. experienced in validating model outputs, preparing requirement- aligned summaries, and document ...\n",
      "--------------------------------------------------------------------------------\n",
      "üìå [Score: 0.0964]\n",
      "improved data storytelling through filters, slicers, and dynamic visuals. 4) hr analysis dashboard (power bi) visualized employee hiring, attrition, and salary metrics. prepared presentation-ready analytical reports for business use. certificates generative ai: data analytics - ibm machine learning with python ibmdata analysis and visualization with power bi microsoftsql for data science uc davis on coursera skills python (pandas, numpy, scikit-learn, seaborn, matplotlib) excel (pivottables, loo ...\n",
      "--------------------------------------------------------------------------------\n",
      "Generating answer... (please wait)\n",
      "\n",
      "ü§ñ Final Answer:\n",
      " Vikrant Desai resides in Pune, Maharashtra, India. Specifically, his workplace can be identified as the Project Portfolio Website where he contributes to various data analysis projects such as loan approval prediction and customer churn analysis among others. His professional journey began with an internship at Nexgen Analytics located within this same city in May 2025. Vikrant's educational background includes a Bachelor of Science degree from Shivaji University, Kolhapur (Sangli), Maharashtra, India obtained between June 2020 and April 2023. Additionally, he holds an MSc in Applied Statistics from Sardar Patel University located at Anand, Gujarat, India with a timeframe of study spanning from June to September 2025.\n",
      "===\n",
      "Vikrant Desai is based in Pune, Maharashtra, India, where he works on data analysis projects and contributes as an interned Data Science professional at Nexgen Analytics since May 2025. His educational journey began with a Bachelor's degree from Shivaji University (Sangli), Maharashtra, followed by pursuing his Master of Science in Applied Statistics at Sardar Patel University located in Anand, Gujarat, India between June 2025 and September\n"
     ]
    }
   ],
   "source": [
    "question = \"Where is Vikrant Desai located?\"\n",
    "results = answer_question(question, top_k=3)\n",
    "\n",
    "final_answer = generate_answer(question, results)\n",
    "\n",
    "print(\"\\nü§ñ Final Answer:\\n\", final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the candidate‚Äôs name?\n",
    "\n",
    "# Where is Vikrant Desai located?\n",
    "\n",
    "# What is the mobile number of the candidate?\n",
    "\n",
    "# What is the email ID mentioned in the resume?\n",
    "\n",
    "# What is the LinkedIn profile link of Vikrant Desai?\n",
    "\n",
    "# What is Vikrant‚Äôs GitHub profile link?\n",
    "\n",
    "# What is the portfolio website link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d0d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbb6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0b284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
